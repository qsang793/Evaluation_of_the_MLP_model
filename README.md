# Evaluation_of_the_MLP_model

## Overview

In the modern context, exploring and optimizing deep learning models have become essential for developing artificial intelligence applications. My goal in this series of experiments is to evaluate the performance of the Multi-Layer Perceptron (MLP) model when applying different activation functions, data normalization methods, and weight initialization strategies. Specifically, I will:

1. **Activation Functions**: Experiment with the Tanh activation function on the Fashion-MNIST and Cifar10 datasets, and compare the results with the Sigmoid function to determine the optimal activation function for high model accuracy.

2. **Data Normalization**: Apply various data normalization methods to a fixed model to analyze their impact on the overall performance and learning speed of the model.

3. **Weight Initialization**: Conduct experiments with three weight initialization strategies - Gaussian with a mean of 0 and a standard deviation of 0.01, Glorot, and He - to evaluate their impact on the learning process and the accuracy of the model.

Through these experiments, I aim to identify the critical factors for optimizing MLP models, contributing to the broader field of deep learning, and supporting the development of more robust and efficient machine learning models.